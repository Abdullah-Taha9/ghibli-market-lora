# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RCJOm0nQaWAOhFCWl7Zne9vRvVB7vtF8
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install diffusers peft torch>2 torchvision safetensors pathlib PIL tqdm

# Commented out IPython magic to ensure Python compatibility.
# %pip show PIL

"""## imports and dependencies"""

import os
from pathlib import Path
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from tqdm import tqdm
import logging

from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline
from transformers import CLIPTokenizer, CLIPTextModel
from peft import LoraConfig

import safetensors.torch as safetensors

"""## Config and dataset loader"""

class Config:
    # MODEL_NAME = "runwayml/stable-diffusion-v1-5"
    MODEL_NAME = "/home/hpc/v123be/v123be39/dl/sd15"
    INSTANCE_TOKEN = "<sks>"

    RESOLUTION = 512
    BATCH_SIZE = 2
    GRAD_ACCUM = 8
    LR = 1e-4
    MAX_STEPS = 4000

    LORA_RANK = 16
    LORA_ALPHA = 64

    DATA_DIR = "data/512"
    OUTPUT_DIR = "lora_out"
    LOG_STEPS = 50
    SAVE_STEPS = 4000


# ---------------- DATASET ----------------
class StyleDataset(Dataset):
    def __init__(self, data_dir, instance_token, resolution=512):
        self.paths = list(Path(data_dir).glob("*"))
        if len(self.paths) == 0:
            raise ValueError(f"No images found in {data_dir}")
        print(f"Found {len(self.paths)} training images")

        self.prompt = f"a busy market, in {instance_token} style"
        self.transforms = transforms.Compose([
            transforms.Resize((resolution, resolution)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ])

    def __len__(self): return len(self.paths)

    def __getitem__(self, i):
        img = Image.open(self.paths[i]).convert("RGB")
        return {"pixel_values": self.transforms(img), "prompt": self.prompt}


def collate_fn(examples):
    return {
        "pixel_values": torch.stack([e["pixel_values"] for e in examples]),
        "prompts": [e["prompt"] for e in examples],
    }


def create_dataloader():
    ds = StyleDataset(Config.DATA_DIR, Config.INSTANCE_TOKEN, Config.RESOLUTION)
    return DataLoader(ds, batch_size=Config.BATCH_SIZE, shuffle=True, collate_fn=collate_fn)

"""## Utils"""

def setup_logging():
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    return logging.getLogger(__name__)


def add_token(tokenizer, text_encoder, token):
    tokenizer.add_tokens(token)
    text_encoder.resize_token_embeddings(len(tokenizer))
    tok_id = tokenizer.convert_tokens_to_ids(token)

    with torch.no_grad():
        text_encoder.get_input_embeddings().weight[tok_id] = (
            text_encoder.get_input_embeddings().weight[-2:-1].mean(dim=0)
        )
    return tok_id


def encode_prompt(text_encoder, tokenizer, prompts, device):
    inputs = tokenizer(
        prompts, padding="max_length", truncation=True, max_length=77, return_tensors="pt"
    ).to(device)
    return text_encoder(input_ids=inputs.input_ids).last_hidden_state

"""## Training"""

def train():
    logger = setup_logging()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load models
    logger.info("Loading models...")
    tokenizer = CLIPTokenizer.from_pretrained(Config.MODEL_NAME, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(Config.MODEL_NAME, subfolder="text_encoder").to(device)
    vae = AutoencoderKL.from_pretrained(Config.MODEL_NAME, subfolder="vae").to(device)
    unet = UNet2DConditionModel.from_pretrained(Config.MODEL_NAME, subfolder="unet").to(device)
    noise_scheduler = DDPMScheduler.from_pretrained(Config.MODEL_NAME, subfolder="scheduler")
    vae.requires_grad_(False)

    # Freeze base weights before adding LoRA adapters
    unet.requires_grad_(False)
    text_encoder.requires_grad_(False)

    # Add special token
    add_token(tokenizer, text_encoder, Config.INSTANCE_TOKEN)

    # Add LoRA adapters
    unet_lora_config = LoraConfig(
        r=Config.LORA_RANK,
        lora_alpha=Config.LORA_ALPHA,
        init_lora_weights="gaussian",
        target_modules=["to_q", "to_k", "to_v", "to_out.0"],
    )
    unet.add_adapter(unet_lora_config)

    text_lora_config = LoraConfig(
        r=Config.LORA_RANK,
        lora_alpha=Config.LORA_ALPHA,
        init_lora_weights="gaussian",
        target_modules=["q_proj", "k_proj", "v_proj", "out_proj"],
    )
    text_encoder.add_adapter(text_lora_config)


    # Optimizer: only LoRA params
    optimizer = torch.optim.AdamW(
        list(filter(lambda p: p.requires_grad, unet.parameters())) +
        list(filter(lambda p: p.requires_grad, text_encoder.parameters())),
        lr=Config.LR
    )

    dataloader = create_dataloader()

    # Training loop
    logger.info(f"Training {Config.MAX_STEPS} steps...")
    progress = tqdm(range(Config.MAX_STEPS))
    step = 0

    unet.train()
    text_encoder.train()

    while step < Config.MAX_STEPS:
        for batch in dataloader:
            if step >= Config.MAX_STEPS:
                break

            imgs = batch["pixel_values"].to(device)
            prompts = batch["prompts"]

            # Latents
            with torch.no_grad():
                latents = vae.encode(imgs).latent_dist.sample() * 0.18215

            noise = torch.randn_like(latents)
            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device)
            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

            # Prompt embeds
            prompt_embeds = encode_prompt(text_encoder, tokenizer, prompts, device)

            # Predict noise
            noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=prompt_embeds).sample
            loss = F.mse_loss(noise_pred, noise)
            loss.backward()

            if (step + 1) % Config.GRAD_ACCUM == 0:
                optimizer.step()
                optimizer.zero_grad()

            step += 1
            progress.update(1)

            if step % Config.LOG_STEPS == 0:
                logger.info(f"Step {step}, Loss {loss.item():.6f}")

            if step % Config.SAVE_STEPS == 0:
                unet.save_pretrained(f"{Config.OUTPUT_DIR}/unet-{step}")
                text_encoder.save_pretrained(f"{Config.OUTPUT_DIR}/text_encoder-{step}")
                logger.info(f"Saved checkpoint at step {step}")

    def save_lora_weights(unet, text_encoder, save_path):
        lora_state_dict = {}

        # UNet LoRA weights
        for key, value in unet.state_dict().items():
            if "lora" in key.lower():
                lora_state_dict[f"unet.{key}"] = value.detach().cpu()

        # Text encoder LoRA weights
        for key, value in text_encoder.state_dict().items():
            if "lora" in key.lower():
                lora_state_dict[f"text_encoder.{key}"] = value.detach().cpu()

        # Save single safetensors file
        safetensors.save_file(lora_state_dict, save_path)
        print(f"âœ… Saved all LoRA weights to {save_path}")

    final_path = Path(Config.OUTPUT_DIR) / "pytorch_lora_weights.safetensors"
    save_lora_weights(unet, text_encoder, final_path)   

    # Save final weights
    # unet.save_pretrained(f"{Config.OUTPUT_DIR}/unet-final")
    # text_encoder.save_pretrained(f"{Config.OUTPUT_DIR}/text_encoder-final")
    logger.info("Training completed!")

"""## run pipeline"""

if __name__ == "__main__":
    Path(Config.OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
    train()
